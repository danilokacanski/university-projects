{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Loan Approval Classification",
   "id": "3cfff2c0c4fc532d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Introduction",
   "id": "afcf409de1be1a23"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this practice exercise, you will explore and analyze a dataset related to loan approval decisions. This dataset contains various features that might influence the likelihood of a loan being approved, such as personal income, employment experience, credit history and more. Your goal is to **build a deep learning model** using a neural network to predict whether a loan will be approved based on these features. This will give you practical experience in **data preprocessing**, **feature selection** and **model training** using PyTorch.\n",
    "\n",
    "The dataset can be downloaded from Kaggle:[Loan Approval Classification Data](https://www.kaggle.com/datasets/taweilo/loan-approval-classification-data).\n",
    "\n",
    "This exercise will be divided into two parts:\n",
    "\n",
    "- **Part 1**: You will use **all available features** to build and train the model.\n",
    "- **Part 2**: You will refine the model by using only those **features that show a strong correlation** (greater than 25%) with the target variable, which should help in improving the model's performance. "
   ],
   "id": "c8ee1551061f74d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![Alt Text](./images/lp2.jpg)",
   "id": "504606d343f2ec44"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Libraries",
   "id": "7a141bbd566464bb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code imports necessary libraries and modules. Pandas is used for data manipulation, **PyTorch** for building and training the neural network and **scikit-learn** for data scaling and splitting.",
   "id": "98dff079ee9e51ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T15:22:45.313963Z",
     "start_time": "2024-12-20T15:22:45.296452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ],
   "id": "8fc82f5d94a889cf",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Data and Perform One-hot Encoding",
   "id": "9e785a3fa0b8c888"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this cell, the loan dataset is loaded from a CSV file. It also performs **one-hot encoding** on categorical variables to convert them into a format that can be processed by neural networks.",
   "id": "d3a2aa734a5582fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T15:22:43.699960Z",
     "start_time": "2024-12-20T15:22:43.415384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_path = './data/loan_data.csv'\n",
    "loan_data = pd.read_csv(data_path)\n",
    "\n",
    "# One-hot encode non-numerical data\n",
    "loan_data_encoded = pd.get_dummies(loan_data)"
   ],
   "id": "d0a374cc1d08cc2d",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T11:55:11.443290Z",
     "start_time": "2024-12-20T11:55:11.386187Z"
    }
   },
   "cell_type": "code",
   "source": "loan_data_encoded",
   "id": "a9d556c1271fbf37",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       person_age  person_income  person_emp_exp  loan_amnt  loan_int_rate  \\\n",
       "0            22.0        71948.0               0    35000.0          16.02   \n",
       "1            21.0        12282.0               0     1000.0          11.14   \n",
       "2            25.0        12438.0               3     5500.0          12.87   \n",
       "3            23.0        79753.0               0    35000.0          15.23   \n",
       "4            24.0        66135.0               1    35000.0          14.27   \n",
       "...           ...            ...             ...        ...            ...   \n",
       "44995        27.0        47971.0               6    15000.0          15.66   \n",
       "44996        37.0        65800.0              17     9000.0          14.07   \n",
       "44997        33.0        56942.0               7     2771.0          10.02   \n",
       "44998        29.0        33164.0               4    12000.0          13.23   \n",
       "44999        24.0        51609.0               1     6665.0          17.05   \n",
       "\n",
       "       loan_percent_income  cb_person_cred_hist_length  credit_score  \\\n",
       "0                     0.49                         3.0           561   \n",
       "1                     0.08                         2.0           504   \n",
       "2                     0.44                         3.0           635   \n",
       "3                     0.44                         2.0           675   \n",
       "4                     0.53                         4.0           586   \n",
       "...                    ...                         ...           ...   \n",
       "44995                 0.31                         3.0           645   \n",
       "44996                 0.14                        11.0           621   \n",
       "44997                 0.05                        10.0           668   \n",
       "44998                 0.36                         6.0           604   \n",
       "44999                 0.13                         3.0           628   \n",
       "\n",
       "       loan_status  person_gender_female  ...  person_home_ownership_OWN  \\\n",
       "0                1                  True  ...                      False   \n",
       "1                0                  True  ...                       True   \n",
       "2                1                  True  ...                      False   \n",
       "3                1                  True  ...                      False   \n",
       "4                1                 False  ...                      False   \n",
       "...            ...                   ...  ...                        ...   \n",
       "44995            1                 False  ...                      False   \n",
       "44996            1                  True  ...                      False   \n",
       "44997            1                 False  ...                      False   \n",
       "44998            1                 False  ...                      False   \n",
       "44999            1                 False  ...                      False   \n",
       "\n",
       "       person_home_ownership_RENT  loan_intent_DEBTCONSOLIDATION  \\\n",
       "0                            True                          False   \n",
       "1                           False                          False   \n",
       "2                           False                          False   \n",
       "3                            True                          False   \n",
       "4                            True                          False   \n",
       "...                           ...                            ...   \n",
       "44995                        True                          False   \n",
       "44996                        True                          False   \n",
       "44997                        True                           True   \n",
       "44998                        True                          False   \n",
       "44999                        True                           True   \n",
       "\n",
       "       loan_intent_EDUCATION  loan_intent_HOMEIMPROVEMENT  \\\n",
       "0                      False                        False   \n",
       "1                       True                        False   \n",
       "2                      False                        False   \n",
       "3                      False                        False   \n",
       "4                      False                        False   \n",
       "...                      ...                          ...   \n",
       "44995                  False                        False   \n",
       "44996                  False                         True   \n",
       "44997                  False                        False   \n",
       "44998                   True                        False   \n",
       "44999                  False                        False   \n",
       "\n",
       "       loan_intent_MEDICAL  loan_intent_PERSONAL  loan_intent_VENTURE  \\\n",
       "0                    False                  True                False   \n",
       "1                    False                 False                False   \n",
       "2                     True                 False                False   \n",
       "3                     True                 False                False   \n",
       "4                     True                 False                False   \n",
       "...                    ...                   ...                  ...   \n",
       "44995                 True                 False                False   \n",
       "44996                False                 False                False   \n",
       "44997                False                 False                False   \n",
       "44998                False                 False                False   \n",
       "44999                False                 False                False   \n",
       "\n",
       "       previous_loan_defaults_on_file_No  previous_loan_defaults_on_file_Yes  \n",
       "0                                   True                               False  \n",
       "1                                  False                                True  \n",
       "2                                   True                               False  \n",
       "3                                   True                               False  \n",
       "4                                   True                               False  \n",
       "...                                  ...                                 ...  \n",
       "44995                               True                               False  \n",
       "44996                               True                               False  \n",
       "44997                               True                               False  \n",
       "44998                               True                               False  \n",
       "44999                               True                               False  \n",
       "\n",
       "[45000 rows x 28 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_age</th>\n",
       "      <th>person_income</th>\n",
       "      <th>person_emp_exp</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>loan_int_rate</th>\n",
       "      <th>loan_percent_income</th>\n",
       "      <th>cb_person_cred_hist_length</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>loan_status</th>\n",
       "      <th>person_gender_female</th>\n",
       "      <th>...</th>\n",
       "      <th>person_home_ownership_OWN</th>\n",
       "      <th>person_home_ownership_RENT</th>\n",
       "      <th>loan_intent_DEBTCONSOLIDATION</th>\n",
       "      <th>loan_intent_EDUCATION</th>\n",
       "      <th>loan_intent_HOMEIMPROVEMENT</th>\n",
       "      <th>loan_intent_MEDICAL</th>\n",
       "      <th>loan_intent_PERSONAL</th>\n",
       "      <th>loan_intent_VENTURE</th>\n",
       "      <th>previous_loan_defaults_on_file_No</th>\n",
       "      <th>previous_loan_defaults_on_file_Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.0</td>\n",
       "      <td>71948.0</td>\n",
       "      <td>0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>16.02</td>\n",
       "      <td>0.49</td>\n",
       "      <td>3.0</td>\n",
       "      <td>561</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.0</td>\n",
       "      <td>12282.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>11.14</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>504</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25.0</td>\n",
       "      <td>12438.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5500.0</td>\n",
       "      <td>12.87</td>\n",
       "      <td>0.44</td>\n",
       "      <td>3.0</td>\n",
       "      <td>635</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.0</td>\n",
       "      <td>79753.0</td>\n",
       "      <td>0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>15.23</td>\n",
       "      <td>0.44</td>\n",
       "      <td>2.0</td>\n",
       "      <td>675</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.0</td>\n",
       "      <td>66135.0</td>\n",
       "      <td>1</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>14.27</td>\n",
       "      <td>0.53</td>\n",
       "      <td>4.0</td>\n",
       "      <td>586</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44995</th>\n",
       "      <td>27.0</td>\n",
       "      <td>47971.0</td>\n",
       "      <td>6</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>15.66</td>\n",
       "      <td>0.31</td>\n",
       "      <td>3.0</td>\n",
       "      <td>645</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44996</th>\n",
       "      <td>37.0</td>\n",
       "      <td>65800.0</td>\n",
       "      <td>17</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>14.07</td>\n",
       "      <td>0.14</td>\n",
       "      <td>11.0</td>\n",
       "      <td>621</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44997</th>\n",
       "      <td>33.0</td>\n",
       "      <td>56942.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2771.0</td>\n",
       "      <td>10.02</td>\n",
       "      <td>0.05</td>\n",
       "      <td>10.0</td>\n",
       "      <td>668</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44998</th>\n",
       "      <td>29.0</td>\n",
       "      <td>33164.0</td>\n",
       "      <td>4</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>13.23</td>\n",
       "      <td>0.36</td>\n",
       "      <td>6.0</td>\n",
       "      <td>604</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44999</th>\n",
       "      <td>24.0</td>\n",
       "      <td>51609.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6665.0</td>\n",
       "      <td>17.05</td>\n",
       "      <td>0.13</td>\n",
       "      <td>3.0</td>\n",
       "      <td>628</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45000 rows × 28 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Part 1: Utilizing All Features",
   "id": "f0a79a92ebfbf43c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This cell prepares the data by separating the **features** (independent variables) and the **target** (dependent variable *loan_status*).",
   "id": "4c726ecee6bea90f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T15:22:53.964102Z",
     "start_time": "2024-12-20T15:22:53.952094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "features = loan_data_encoded.drop('loan_status', axis=1)\n",
    "target = loan_data_encoded['loan_status']"
   ],
   "id": "44734df87182ceee",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After executing the cell that separates the features and the target variable for the entire dataset, you have the flexibility to directly proceed to the \"*Splitting and Scaling the Data*\" section. This allows you to **test the model using all available features** without applying any feature selection based on correlation. When you're ready to **experiment with a potentially more efficient model** that only includes highly correlated features, you can return and run the intermediate cells that perform **feature selection** based on the correlation threshold. This approach lets you **compare the performance** of the neural network when using all features versus using a subset of features with strong correlations to the target variable, enhancing your understanding of feature importance and model optimization.",
   "id": "112a185aacbccac3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Part 2: Using Highly Correlated Features",
   "id": "64877763f9c6cc87"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " This cell identifies and selects features that have a **strong correlation** (absolute value greater than 25%) with the target variable *loan_status*. The selected features are expected to have more impact on the model's predictions.",
   "id": "f4ceec7985b62160"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T15:27:35.858298Z",
     "start_time": "2024-12-20T15:27:35.791926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "correlation_matrix = loan_data_encoded.corr()\n",
    "target_correlation = correlation_matrix['loan_status']\n",
    "\n",
    "significant_features = target_correlation[abs(target_correlation) > 0.25].index.tolist()\n",
    "significant_features.remove('loan_status')  # Exclude the target variable itself\n",
    "print(significant_features)\n",
    "filtered_data = loan_data_encoded[significant_features + ['loan_status']]"
   ],
   "id": "3f81637a46ed05a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loan_int_rate', 'loan_percent_income', 'person_home_ownership_RENT', 'previous_loan_defaults_on_file_No', 'previous_loan_defaults_on_file_Yes']\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Preparation for Training",
   "id": "40b18589c3d49d97"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here, the dataset is split into **filtered features** (independent variables) and the target (dependent variable, *loan_status*).",
   "id": "15be80f2a58f4ddd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T15:23:05.929250Z",
     "start_time": "2024-12-20T15:23:05.897954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filtered_data = loan_data[['loan_status', 'person_income', 'credit_score', 'loan_percent_income','loan_intent']]\n",
    "filtered_data = pd.get_dummies(filtered_data,columns=['loan_intent'])"
   ],
   "id": "608087cbe8c329fb",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T15:23:09.317671Z",
     "start_time": "2024-12-20T15:23:09.304354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "features = filtered_data.drop('loan_status', axis=1)\n",
    "target = filtered_data['loan_status']"
   ],
   "id": "2c22fd9af709022",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T13:57:59.755506Z",
     "start_time": "2024-12-20T13:57:59.733934Z"
    }
   },
   "cell_type": "code",
   "source": "print(target)",
   "id": "a5752feef8934bbd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        1\n",
      "1        0\n",
      "2        1\n",
      "3        1\n",
      "4        1\n",
      "        ..\n",
      "44995    1\n",
      "44996    1\n",
      "44997    1\n",
      "44998    1\n",
      "44999    1\n",
      "Name: loan_status, Length: 45000, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Splitting and Scaling the Data",
   "id": "cc9f8991f2f7049f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The data is divided into training and validation sets. The features are then **scaled** using standard scaling, which normalizes the data by subtracting the mean and scaling to unit variance, facilitating more efficient training of neural networks.",
   "id": "f1b7e10cd3118840"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T15:23:14.164420Z",
     "start_time": "2024-12-20T15:23:14.062010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)"
   ],
   "id": "cfbbfd79a59bbc90",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Converting Data to PyTorch Tensors and Creating Data Loaders",
   "id": "dfd0ba2ee5855abc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This cell converts the scaled training and validation sets into PyTorch **tensors** and creates **data loaders**. Data loaders facilitate batch processing, shuffling for the training set and sequential access for the validation set.",
   "id": "bd607e9ed54a7c24"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T15:24:43.104844Z",
     "start_time": "2024-12-20T15:24:43.089542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)"
   ],
   "id": "9307cd6f83798a45",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T14:00:24.019918Z",
     "start_time": "2024-12-20T14:00:24.004660Z"
    }
   },
   "cell_type": "code",
   "source": "X_train_tensor",
   "id": "ca7dc33442143bae",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1942, -0.0324,  0.1191,  ..., -0.4834,  2.2294, -0.4573],\n",
       "        [-0.0900,  0.5247,  0.2339,  ..., -0.4834, -0.4486,  2.1866],\n",
       "        [ 2.8282,  1.0022, -0.7999,  ..., -0.4834,  2.2294, -0.4573],\n",
       "        ...,\n",
       "        [-0.2610,  0.3854,  0.4637,  ..., -0.4834, -0.4486,  2.1866],\n",
       "        [ 1.1450, -0.5697, -0.3404,  ..., -0.4834, -0.4486, -0.4573],\n",
       "        [ 1.5225,  0.6242, -0.7999,  ..., -0.4834,  2.2294, -0.4573]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Defining the NN Model",
   "id": "ff27e6a1d9963b90"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A **sequential neural network model** is defined here, featuring several layers with ReLU activations and a final sigmoid layer to output a probability for binary classification.",
   "id": "ac0dc8029385b71e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T15:24:46.104548Z",
     "start_time": "2024-12-20T15:24:46.092525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(features.shape[1], 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 1),\n",
    "    nn.Sigmoid()\n",
    ")"
   ],
   "id": "d8f67f20206cb840",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setting Loss Function and Optimizer",
   "id": "e88f16d0eddb9bec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Binary Cross-Entropy Loss** is used as the loss function, which is suitable for binary classification problems. The **Adam** optimizer is used to adjust model weights based on this loss.",
   "id": "4fd598ebb5bb4f62"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T15:24:48.456404Z",
     "start_time": "2024-12-20T15:24:48.446635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "criterion = nn.BCEWithLogitsLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0015)"
   ],
   "id": "c9debf6196726c4a",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training and Validation of the Model",
   "id": "fe1a8c2757426ac8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This final code cell contains the training loop where the model is trained for a specified number of epochs, and its performance is evaluated on both the training and validation datasets. Loss and accuracy metrics are calculated and printed for each epoch to monitor the training progress and effectiveness of the model.",
   "id": "7817f1b9a81226e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T15:26:14.613503Z",
     "start_time": "2024-12-20T15:24:50.782624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 40\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = model(inputs).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        predicted = nn.functional.sigmoid(outputs) > 0.5\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}, Train Acc: {train_accuracy}%')\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_train_tensor)\n",
    "    y_pred = nn.functional.sigmoid(outputs) > 0.5\n",
    "    y_pred_correct = y_pred.type(torch.float32) == y_train_tensor\n",
    "    print(y_pred_correct.type(torch.float32).mean())"
   ],
   "id": "68fa22d00ab04d51",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6931177377700806, Train Acc: 22.81388888888889%\n",
      "Epoch 2, Loss: 0.6735822558403015, Train Acc: 37.205555555555556%\n",
      "Epoch 3, Loss: 0.6763483285903931, Train Acc: 71.11111111111111%\n",
      "Epoch 4, Loss: 0.681469202041626, Train Acc: 74.46944444444445%\n",
      "Epoch 5, Loss: 0.6768664121627808, Train Acc: 74.73611111111111%\n",
      "Epoch 6, Loss: 0.6877772212028503, Train Acc: 76.1361111111111%\n",
      "Epoch 7, Loss: 0.681305468082428, Train Acc: 76.65555555555555%\n",
      "Epoch 8, Loss: 0.6457577347755432, Train Acc: 76.08333333333333%\n",
      "Epoch 9, Loss: 0.703721284866333, Train Acc: 76.51388888888889%\n",
      "Epoch 10, Loss: 0.6459529995918274, Train Acc: 76.34722222222223%\n",
      "Epoch 11, Loss: 0.6578864455223083, Train Acc: 76.21111111111111%\n",
      "Epoch 12, Loss: 0.6747952103614807, Train Acc: 76.96388888888889%\n",
      "Epoch 13, Loss: 0.6579850912094116, Train Acc: 76.91666666666667%\n",
      "Epoch 14, Loss: 0.6931791305541992, Train Acc: 77.23333333333333%\n",
      "Epoch 15, Loss: 0.6459723114967346, Train Acc: 78.30833333333334%\n",
      "Epoch 16, Loss: 0.6593371033668518, Train Acc: 78.69444444444444%\n",
      "Epoch 17, Loss: 0.635075569152832, Train Acc: 79.54444444444445%\n",
      "Epoch 18, Loss: 0.6933004856109619, Train Acc: 79.52777777777777%\n",
      "Epoch 19, Loss: 0.7150192260742188, Train Acc: 79.96388888888889%\n",
      "Epoch 20, Loss: 0.6648094654083252, Train Acc: 80.06388888888888%\n",
      "Epoch 21, Loss: 0.645135223865509, Train Acc: 80.31111111111112%\n",
      "Epoch 22, Loss: 0.6418477892875671, Train Acc: 80.77222222222223%\n",
      "Epoch 23, Loss: 0.6774864792823792, Train Acc: 80.43888888888888%\n",
      "Epoch 24, Loss: 0.7020677924156189, Train Acc: 79.88888888888889%\n",
      "Epoch 25, Loss: 0.6351975798606873, Train Acc: 80.46111111111111%\n",
      "Epoch 26, Loss: 0.6786606907844543, Train Acc: 80.55277777777778%\n",
      "Epoch 27, Loss: 0.6891205906867981, Train Acc: 80.98611111111111%\n",
      "Epoch 28, Loss: 0.7008712887763977, Train Acc: 81.15%\n",
      "Epoch 29, Loss: 0.6464959979057312, Train Acc: 80.98055555555555%\n",
      "Epoch 30, Loss: 0.669620931148529, Train Acc: 81.13055555555556%\n",
      "Epoch 31, Loss: 0.6769120097160339, Train Acc: 81.13333333333334%\n",
      "Epoch 32, Loss: 0.6568459868431091, Train Acc: 81.2611111111111%\n",
      "Epoch 33, Loss: 0.6734599471092224, Train Acc: 81.775%\n",
      "Epoch 34, Loss: 0.6778185367584229, Train Acc: 81.18888888888888%\n",
      "Epoch 35, Loss: 0.6909394264221191, Train Acc: 81.17222222222222%\n",
      "Epoch 36, Loss: 0.6930674314498901, Train Acc: 81.01388888888889%\n",
      "Epoch 37, Loss: 0.699937641620636, Train Acc: 81.28055555555555%\n",
      "Epoch 38, Loss: 0.6687421798706055, Train Acc: 81.55833333333334%\n",
      "Epoch 39, Loss: 0.6599553227424622, Train Acc: 81.4888888888889%\n",
      "Epoch 40, Loss: 0.6866954565048218, Train Acc: 81.40555555555555%\n",
      "tensor(0.6834)\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Comparative Analysis of Model Performance",
   "id": "492bd27838aa8e30"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In our hands-on exercise on loan approval predictions using PyTorch, we observed a significant result concerning model accuracy. When comparing the performance of the two models—one trained with all available features and the other with only the features highly correlated (greater than 25%) with the target variable—a notable pattern emerged:\n",
    "\n",
    "**Models' Performance Insight**:\n",
    "\n",
    "- The model trained with **all available features** demonstrated **higher accuracy** on the **validation dataset** compared to the model that used only the highly correlated features."
   ],
   "id": "52b1373524f8279"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Why Does Using All Features Sometimes Lead to Better Performance?",
   "id": "bf22fbaa1dc8b3b5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. **Comprehensive Data Representation**: When all features are used, the model has access to a broader spectrum of information. This comprehensive data can include subtle but valuable patterns that might be overlooked when only selecting features based on correlation.\n",
    "\n",
    "2. **Interactions Between Features**: Some features may not show a strong individual correlation with the target variable but could be influential when combined with other features. Using all available data allows the model to capture these complex interactions.\n",
    "\n",
    "3. **Reduced Risk of Overfitting to Correlation**: Focusing solely on highly correlated features can sometimes lead to models that are too tailored to the specifics of the training data, including potential noise and anomalies specific to that subset of features. This can affect the model's ability to generalize to new, unseen data."
   ],
   "id": "6f9f25d21dfd0eb1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Conclusion",
   "id": "79d1d2ecb242d85f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The findings suggest that while feature selection based on correlation can **simplify the model and reduce training time**, it **does not always guarantee** improved performance. This underscores the importance of experimenting with different feature subsets and using validation performance as a benchmark to judge the best approach. In this case, utilizing the complete dataset provided a more accurate and robust model for predicting loan approvals, highlighting that more data often leads to better learning and prediction capabilities.\n",
    "\n",
    "In practice, it's beneficial to **evaluate models both with and without feature selection** to determine the **optimal** approach for specific datasets and objectives. This exercise not only enhances your technical skills but also deepens your understanding of strategic model development in data science."
   ],
   "id": "656fd81b26c1f0db"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

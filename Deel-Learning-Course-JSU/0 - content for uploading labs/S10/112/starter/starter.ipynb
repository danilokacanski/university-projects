{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "print(\"Running on device:\", device)\n",
    "\n",
    "mnist_train = datasets.FashionMNIST(root='./data', download=True, train=True, transform=ToTensor())\n",
    "mnist_test = datasets.FashionMNIST(root='./data', download=True, train=False, transform=ToTensor())\n",
    "\n",
    "train_dataloader = DataLoader(mnist_train, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(mnist_test, batch_size=32, shuffle=True)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Sequential(\n",
    "        nn.Conv2d(1, 32, kernel_size=(3, 3), padding=1, padding_mode=\"reflect\"),\n",
    "        nn.MaxPool2d(kernel_size=2),\n",
    "        nn.ReLU(),\n",
    "        # TODO: Add dropout here to regularize the first conv block (e.g., nn.Dropout(0.1))\n",
    "    ),\n",
    "    nn.Sequential(\n",
    "        nn.Conv2d(32, 64, kernel_size=(3, 3), padding=1, padding_mode=\"reflect\"),\n",
    "        nn.MaxPool2d(kernel_size=2),\n",
    "        nn.ReLU(),\n",
    "        # TODO: Add dropout here to regularize the second conv block (e.g., nn.Dropout(0.1))\n",
    "    ),\n",
    "    nn.Flatten(),\n",
    "    nn.Sequential(\n",
    "        nn.Linear(64 * 7 * 7, 1000),\n",
    "        nn.ReLU(),\n",
    "        # TODO: Add dropout after the first linear layer (e.g., nn.Dropout(0.3))\n",
    "        nn.Linear(1000, 100),\n",
    "        nn.ReLU(),\n",
    "        # TODO: Add dropout before the final classifier (e.g., nn.Dropout(0.5))\n",
    "        nn.Linear(100, 10)\n",
    "    )\n",
    ").to(device)\n",
    "print(model)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for i in range(0, 10):\n",
    "    model.train()\n",
    "\n",
    "    loss_sum = 0\n",
    "    for X, y in train_dataloader:\n",
    "        y = F.one_hot(y, num_classes=10).type(torch.float32).to(device)\n",
    "        X = X.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = loss_fn(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "    print(loss_sum)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    accurate = 0\n",
    "    total = 0\n",
    "    for X, y in test_dataloader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        outputs = nn.functional.softmax(model(X), dim=1)\n",
    "        correct_pred = (y == outputs.max(dim=1).indices)\n",
    "        total += correct_pred.size(0)\n",
    "        accurate += correct_pred.type(torch.int).sum().item()\n",
    "    print(\"Accuracy on validation data:\", accurate / total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

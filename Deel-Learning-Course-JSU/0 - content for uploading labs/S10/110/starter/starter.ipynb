{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "print(\"Running on device:\", device)\n",
    "\n",
    "mnist_train = datasets.FashionMNIST(root='./data', download=True, train=True, transform=ToTensor())\n",
    "mnist_test = datasets.FashionMNIST(root='./data', download=True, train=False, transform=ToTensor())\n",
    "\n",
    "train_dataloader = DataLoader(mnist_train, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(mnist_test, batch_size=32, shuffle=True)\n",
    "\n",
    "# TODO: Start from the previous small-CNN and scale it up:\n",
    "#  1) Change the first Conv2d from (1 -> 3) to (1 -> 32) with the same kernel/padding.\n",
    "#  2) Change the second Conv2d from (3 -> 6) to (32 -> 64), same kernel/padding.\n",
    "#  3) Keep the two MaxPool2d(2) layers so spatial size goes 28->14->7; channels end at 64.\n",
    "#  4) Update the first Linear in_features from 294 to 64*7*7 and make a deeper head:\n",
    "#    Linear(64*7*7 -> 1000) -> ReLU -> Linear(1000 -> 100) -> ReLU -> Linear(100 -> 10).\n",
    "#  5) Leave the training and evaluation loops as-is.\n",
    "model = nn.Sequential(\n",
    "    nn.Sequential(\n",
    "        nn.Conv2d(1, 3, kernel_size=(3, 3), padding=1, padding_mode=\"reflect\"),\n",
    "        nn.MaxPool2d(kernel_size=2),\n",
    "        nn.ReLU()\n",
    "    ),\n",
    "    nn.Sequential(\n",
    "        nn.Conv2d(3, 6, kernel_size=(3, 3), padding=1, padding_mode=\"reflect\"),\n",
    "        nn.MaxPool2d(kernel_size=2),\n",
    "        nn.ReLU(),\n",
    "    ),\n",
    "    nn.Flatten(),\n",
    "    nn.Sequential(\n",
    "        nn.Linear(294, 100),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100, 10)\n",
    "    )\n",
    ").to(device)\n",
    "print(model)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for i in range(0, 10):\n",
    "    model.train()\n",
    "\n",
    "    loss_sum = 0\n",
    "    for X, y in train_dataloader:\n",
    "        y = F.one_hot(y, num_classes=10).type(torch.float32).to(device)\n",
    "        X = X.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = loss_fn(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_sum+=loss.item()\n",
    "    print(loss_sum)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    accurate = 0\n",
    "    total = 0\n",
    "    for X, y in test_dataloader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        outputs = nn.functional.softmax(model(X), dim=1)\n",
    "        correct_pred = (y == outputs.max(dim=1).indices)\n",
    "        total+=correct_pred.size(0)\n",
    "        accurate+=correct_pred.type(torch.int).sum().item()\n",
    "    print(\"Accuracy on validation data:\", accurate / total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

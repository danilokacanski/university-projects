{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-25T16:35:54.627708Z",
     "start_time": "2025-08-25T16:35:22.263555Z"
    }
   },
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "mnist_train = datasets.FashionMNIST(root='./data', download=True, train=True, transform=ToTensor())\n",
    "mnist_test = datasets.FashionMNIST(root='./data', download=True, train=False, transform=ToTensor())\n",
    "\n",
    "train_dataloader = DataLoader(mnist_train, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(mnist_test, batch_size=32, shuffle=True)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 3, kernel_size=(3, 3), padding=1, padding_mode=\"reflect\"),\n",
    "    # Insert a MaxPool2d layer with kernel_size=2 right after the conv\n",
    "    nn.MaxPool2d(kernel_size=2),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),\n",
    "    # Because of pooling, update the first Linear's in_features from 2352 to 588\n",
    "    nn.Linear(588, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 10)\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for i in range(0, 10):\n",
    "    model.train()\n",
    "\n",
    "    loss_sum = 0\n",
    "    for X, y in train_dataloader:\n",
    "        y = F.one_hot(y, num_classes=10).type(torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = loss_fn(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_sum+=loss.item()\n",
    "    print(loss_sum)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    accurate = 0\n",
    "    total = 0\n",
    "    for X, y in test_dataloader:\n",
    "        outputs = nn.functional.softmax(model(X), dim=1)\n",
    "        correct_pred = (y == outputs.max(dim=1).indices)\n",
    "        total+=correct_pred.size(0)\n",
    "        accurate+=correct_pred.type(torch.int).sum().item()\n",
    "    print(\"Accuracy on validation data:\", accurate / total)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010.5816867724061\n",
      "721.889173116535\n",
      "640.0463021546602\n",
      "582.0099148228765\n",
      "539.3621328733861\n",
      "506.31882278621197\n",
      "477.18520376924425\n",
      "452.79664176516235\n",
      "430.0069045163691\n",
      "411.2172118211165\n",
      "Accuracy on validation data: 0.8946\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

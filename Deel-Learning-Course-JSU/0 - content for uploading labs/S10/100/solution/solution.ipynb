{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-25T16:28:22.473029Z",
     "start_time": "2025-08-25T16:27:48.357561Z"
    }
   },
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "mnist_train = datasets.FashionMNIST(root='./data', download=True, train=True, transform=ToTensor())\n",
    "mnist_test = datasets.FashionMNIST(root='./data', download=True, train=False, transform=ToTensor())\n",
    "\n",
    "train_dataloader = DataLoader(mnist_train, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(mnist_test, batch_size=32, shuffle=True)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 3, kernel_size=(3, 3), padding=1, padding_mode=\"reflect\"),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(2352, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 10)\n",
    ")\n",
    "\n",
    "# Create the loss function for multi-class classification (use CrossEntropyLoss)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Create the optimizer (use Adam with lr=0.001) over model.parameters()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop for 10 epochs:\n",
    "#   - put model in train() mode\n",
    "#   - iterate over train_dataloader:\n",
    "#       * convert labels to one-hot with F.one_hot(y, num_classes=10).float()\n",
    "#       * zero gradients\n",
    "#       * forward pass\n",
    "#       * compute loss\n",
    "#       * backward()\n",
    "#       * optimizer.step()\n",
    "#   - accumulate and print the epoch loss\n",
    "for i in range(0, 10):\n",
    "    model.train()\n",
    "    loss_sum = 0.0\n",
    "    for X, y in train_dataloader:\n",
    "        y_onehot = F.one_hot(y, num_classes=10).type(torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = loss_fn(outputs, y_onehot)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "    print(loss_sum)\n",
    "\n",
    "# Evaluation:\n",
    "#   - model.eval(), wrap in torch.no_grad()\n",
    "#   - iterate over test_dataloader:\n",
    "#       * forward pass\n",
    "#       * apply softmax over dim=1\n",
    "#       * compare argmax with ground-truth y to count correct predictions\n",
    "#   - print validation accuracy as a float in [0, 1]\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    accurate = 0\n",
    "    total = 0\n",
    "    for X, y in test_dataloader:\n",
    "        outputs = nn.functional.softmax(model(X), dim=1)\n",
    "        correct_pred = (y == outputs.max(dim=1).indices)\n",
    "        total += correct_pred.size(0)\n",
    "        accurate += correct_pred.type(torch.int).sum().item()\n",
    "    print(\"Accuracy on validation data:\", accurate / total)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "923.8479181304574\n",
      "617.6442737728357\n",
      "535.0794975254685\n",
      "486.1533539183438\n",
      "441.2763391789049\n",
      "402.9044731967151\n",
      "372.1597451120615\n",
      "341.3199079912156\n",
      "316.3318266160786\n",
      "287.95779645536095\n",
      "Accuracy on validation data: 0.8967\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

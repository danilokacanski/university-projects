{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T10:34:00.849088Z",
     "start_time": "2025-08-23T10:33:52.223414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df = pd.read_csv(\"../starter/SMSSpamCollection\",\n",
    "                 sep=\"\\t\",\n",
    "                 names=[\"type\", \"message\"])\n",
    "\n",
    "df[\"spam\"] = df[\"type\"] == \"spam\"\n",
    "df.drop(\"type\", axis=1, inplace=True)\n",
    "\n",
    "cv = CountVectorizer(max_features=1000)\n",
    "messages = cv.fit_transform(df[\"message\"])\n",
    "\n",
    "X = torch.tensor(messages.todense(), dtype=torch.float32)\n",
    "y = torch.tensor(df[\"spam\"], dtype=torch.float32)\\\n",
    "        .reshape((-1, 1))\n",
    "\n",
    "model = nn.Linear(1000, 1)\n",
    "\n",
    "# Use BCEWithLogitsLoss instead of MSELoss\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Use a higher learning rate (0.02)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.02)\n",
    "\n",
    "for i in range(0, 10000):\n",
    "    # Training pass\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = loss_fn(outputs, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss every 1000 iterations instead of 100\n",
    "    if i % 1000 == 0:\n",
    "        print(loss)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Apply sigmoid to the model outputs before printing\n",
    "    y_pred = nn.functional.sigmoid(model(X))\n",
    "    print(y_pred)\n",
    "    print(y_pred.min())\n",
    "    print(y_pred.max())"
   ],
   "id": "d2229f7a80453f1c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6884, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2251, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1641, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1367, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1206, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1098, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1019, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0958, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0909, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0868, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor([[0.0123],\n",
      "        [0.0203],\n",
      "        [0.9703],\n",
      "        ...,\n",
      "        [0.0143],\n",
      "        [0.0393],\n",
      "        [0.0386]])\n",
      "tensor(1.3198e-10)\n",
      "tensor(0.9999)\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
